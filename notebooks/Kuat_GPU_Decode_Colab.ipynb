{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d146ba",
   "metadata": {},
   "source": [
    "# ðŸš€ Kuat GPU Decode - Performance Test\n",
    "\n",
    "This notebook demonstrates GPU-accelerated VQ decode for Kuat archives.\n",
    "\n",
    "**Key insight**: VQ decode is just a table lookup (`codebook[indices]`), which GPUs do ~100x faster than CPUs.\n",
    "\n",
    "## Setup\n",
    "1. Runtime â†’ Change runtime type â†’ **T4 GPU**\n",
    "2. Run all cells\n",
    "3. (Optional) Upload a real .qvq file to test with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class GPUCodebookDecoder:\n",
    "    \"\"\"\n",
    "    GPU-accelerated VQ decode using PyTorch gather.\n",
    "    \n",
    "    Codebook stays on GPU, indices are tiny transfers.\n",
    "    Decode is just: decoded = codebook[indices]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, codebook_np, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.codebook = torch.from_numpy(codebook_np).to(device)\n",
    "        print(f\"Codebook on {device}: {self.codebook.shape}\")\n",
    "    \n",
    "    def decode_batch(self, indices, width, height, layout=\"NCHW\"):\n",
    "        \"\"\"Decode indices to images on GPU.\"\"\"\n",
    "        batch_size = indices.shape[0]\n",
    "        patch_h, patch_w = 2, 2\n",
    "        patches_h, patches_w = height // patch_h, width // patch_w\n",
    "        \n",
    "        # Small transfer: indices to GPU\n",
    "        idx_gpu = torch.from_numpy(indices.astype(np.int64)).to(self.device)\n",
    "        \n",
    "        # Fast GPU gather\n",
    "        patches = self.codebook[idx_gpu]  # (B, P, 12)\n",
    "        \n",
    "        # Reshape to image\n",
    "        patches = patches.reshape(batch_size, patches_h, patches_w, patch_h, patch_w, 3)\n",
    "        images = patches.permute(0, 1, 3, 2, 4, 5).reshape(batch_size, height, width, 3)\n",
    "        \n",
    "        if layout == \"NCHW\":\n",
    "            images = images.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def decode_float(self, indices, width, height):\n",
    "        \"\"\"Decode to float32 [0,1], ready for model.\"\"\"\n",
    "        return self.decode_batch(indices, width, height, \"NCHW\").float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data (simulates .qvq contents)\n",
    "width, height = 224, 224\n",
    "codebook_size = 65536  # 16-bit\n",
    "patches_per_image = (width // 2) * (height // 2)\n",
    "\n",
    "# Simulated codebook (768 KB on GPU)\n",
    "codebook = np.random.randint(0, 256, (codebook_size, 12), dtype=np.uint8)\n",
    "\n",
    "# Initialize GPU decoder\n",
    "decoder = GPUCodebookDecoder(codebook, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: CPU vs GPU Decode\n",
    "# =============================================================================\n",
    "\n",
    "def bench_cpu(codebook, indices, width, height, n=10):\n",
    "    batch_size = indices.shape[0]\n",
    "    patches_h, patches_w = height // 2, width // 2\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(n):\n",
    "        t0 = time.perf_counter()\n",
    "        patches = codebook[indices]\n",
    "        patches = patches.reshape(batch_size, patches_h, patches_w, 2, 2, 3)\n",
    "        images = np.transpose(patches, (0, 1, 3, 2, 4, 5)).reshape(batch_size, height, width, 3)\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    return np.median(times)\n",
    "\n",
    "def bench_gpu(decoder, indices, width, height, n=10):\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        decoder.decode_batch(indices, width, height)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(n):\n",
    "        t0 = time.perf_counter()\n",
    "        decoder.decode_batch(indices, width, height)\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    return np.median(times)\n",
    "\n",
    "print(\"Batch Size |   CPU img/s |    GPU img/s | Speedup\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for batch_size in [32, 64, 128, 256, 512, 1024]:\n",
    "    indices = np.random.randint(0, codebook_size, (batch_size, patches_per_image), dtype=np.uint16)\n",
    "    \n",
    "    cpu_time = bench_cpu(codebook, indices, width, height)\n",
    "    gpu_time = bench_gpu(decoder, indices, width, height)\n",
    "    \n",
    "    cpu_ips = batch_size / cpu_time\n",
    "    gpu_ips = batch_size / gpu_time\n",
    "    speedup = gpu_ips / cpu_ips\n",
    "    \n",
    "    print(f\"{batch_size:10d} | {cpu_ips:11,.0f} | {gpu_ips:12,.0f} | {speedup:6.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b28390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PEAK THROUGHPUT TEST\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PEAK THROUGHPUT TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_size = 512\n",
    "num_batches = 100\n",
    "indices = np.random.randint(0, codebook_size, (batch_size, patches_per_image), dtype=np.uint16)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    decoder.decode_batch(indices, width, height)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Timed run\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_batches):\n",
    "    images = decoder.decode_batch(indices, width, height)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "total_images = batch_size * num_batches\n",
    "ips = total_images / elapsed\n",
    "\n",
    "print(f\"\\nDecoded {total_images:,} images in {elapsed:.2f}s\")\n",
    "print(f\"Throughput: {ips:,.0f} images/sec\")\n",
    "print(f\"\\nAt 224x224 RGB: {ips * 224 * 224 * 3 / 1e9:.2f} GB/sec of pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# END-TO-END: Simulated Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SIMULATED TRAINING LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple CNN for testing\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 10)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "batch_size = 256\n",
    "num_batches = 50\n",
    "\n",
    "# Simulate training\n",
    "start = time.perf_counter()\n",
    "total_images = 0\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Simulate loading indices from .qvq (this would be mmap read)\n",
    "    indices = np.random.randint(0, codebook_size, (batch_size, patches_per_image), dtype=np.uint16)\n",
    "    labels = torch.randint(0, 10, (batch_size,)).cuda()\n",
    "    \n",
    "    # GPU decode - images already on GPU!\n",
    "    images = decoder.decode_float(indices, 224, 224)\n",
    "    \n",
    "    # Forward + backward\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_images += batch_size\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nTraining loop: {total_images:,} images in {elapsed:.2f}s\")\n",
    "print(f\"Throughput: {total_images/elapsed:,.0f} images/sec (including forward/backward!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93fe9a",
   "metadata": {},
   "source": [
    "## ðŸ“Š Expected Results\n",
    "\n",
    "| Metric | CPU Decode | GPU Decode |\n",
    "|--------|------------|------------|\n",
    "| Decode only | ~5,000 img/s | ~50,000+ img/s |\n",
    "| With training | ~2,000 img/s | ~10,000+ img/s |\n",
    "\n",
    "## ðŸ”‘ Key Benefits\n",
    "\n",
    "1. **No CPUâ†’GPU copy**: Images decoded directly on GPU\n",
    "2. **Tiny transfers**: Only 25 KB of indices per batch (vs 38 MB decoded pixels)\n",
    "3. **Parallel gather**: GPU does billions of lookups per second\n",
    "4. **Memory efficient**: Codebook is only 768 KB on GPU"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
